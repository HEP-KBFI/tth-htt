#!/bin/bash
# File: sbatch-node.hadd.sh.template
# Version: 0.2

# unset JAVA_HOME, because hadoop commands might not work
# this is especially true if one has sourced necessary files for the GRID proxy
echo 'Unsetting JAVA_HOME=$JAVA_HOME'
unset JAVA_HOME

# This value is provided by sbatchManager.py that creates sbatch scripts based this template
echo 'Running script {{ script_file }} (created from template {{ job_template_file }})'


RUNNING_COMMAND="{{ RUNNING_COMMAND }}"
JOB_DIR="{{ job_dir }}/$SLURM_JOBID"

# Runs executable, wrapped into failure wrapper

main() {
    run_failure_wrapped_executable &> "{{ wrapper_log_file }}"
    EXIT_CODE=$?
    return $EXIT_CODE
}


# Executed on node, checks that CVMFS is mounted to this cluster node, or redirects task

run_failure_wrapped_executable() {
    TRY_COUNT=$[TRY_COUNT+1]
    echo "Started (Try $TRY_COUNT)"
    EXIT_CODE=1

    if [[ -f /cvmfs/cms.cern.ch/cmsset_default.sh ]]; then
        echo "Network drive mounted, started running on $HOSTNAME"
        run_wrapped_executable
        EXIT_CODE=$?
    else
        echo "Unable to use node $HOSTNAME, will mark node offline: sudo scontrol update nodename=$HOSTNAME state=drain reason=Testing"
        sudo scontrol update nodename=$HOSTNAME state=drain reason=Testing

        if [[ $TRY_COUNT -lt 3 ]]; then
            echo "Will resubmit job to other node: TRY_COUNT=$TRY_COUNT $RUNNING_COMMAND"
            TRY_COUNT=$TRY_COUNT $RUNNING_COMMAND
            EXIT_CODE=$?
        else
            echo "Maximum tries reached, will not try to resubmit any more. GL & HF"
        fi
    fi

    echo "Delete job directory: rm -r $JOB_DIR"
    rm -r $JOB_DIR

    return $EXIT_CODE
}

cleanup() {
    if [ ! -z "$1" ]; then
        echo "Deleting directory: rm -r $1"
        rm -r "$1";
    fi
}


# Creates scratch dir on cluster node or on /home and runs executable

run_wrapped_executable() {
    EXECUTABLE_LOG_FILE="{{ executable_log_file }}"
    EXECUTABLE_LOG_DIR="`dirname $EXECUTABLE_LOG_FILE`"
    EXECUTABLE_LOG_FILE_NAME="`basename $EXECUTABLE_LOG_FILE`"
    RANDOM_SLEEP={{ random_sleep }}

    echo "Time is: `date`"
    echo "Hostname: `hostname`"
    echo "Current directory: `pwd`"

    echo "Sleeping for $RANDOM_SLEEP seconds"
    sleep $RANDOM_SLEEP

    # Check that input histograms are ok
    check_that_histograms_are_valid.py {{ inputFiles }} &> $EXECUTABLE_LOG_FILE
    EXIT_CODE=$?

    IO_GREP=$(grep "SysError in <TFile::ReadBuffer>" $EXECUTABLE_LOG_FILE | grep "(Input/output error)" | wc -l)
    if [[ $IO_GREP -ne 0 ]]; then
      echo "Encountered I/O error; setting the exit code accordingly";
      EXIT_CODE=3;
    fi

    if [[ $EXIT_CODE -ne 0 ]]; then
	  echo "ERROR: Some of the input histograms are not valid. Will stop execution.";
	  return $EXIT_CODE;
    fi

    echo "Create job directory: mkdir -p $JOB_DIR"
    mkdir -p $JOB_DIR

    echo "Create final log directory: mkdir -p $EXECUTABLE_LOG_DIR"
    mkdir -p $EXECUTABLE_LOG_DIR

    cd $JOB_DIR

    echo "Time is: `date`"

    CMSSW_SEARCH_PATH="$JOB_DIR:{{ cmssw_base_dir }}/src"

    echo "Execute command: {{ exec_name }} {{ command_line_parameter }} &>> $EXECUTABLE_LOG_FILE"
    {{ exec_name }} {{ command_line_parameter }} &>> $EXECUTABLE_LOG_FILE
    EXIT_CODE=$?
    echo "Command {{ exec_name }} exited with code $EXIT_CODE"

    echo "Time is: `date`"

    IO_GREP=$(grep "SysError in <TFile::ReadBuffer>" $EXECUTABLE_LOG_FILE | grep "(Input/output error)" | wc -l)
    if [[ $IO_GREP -ne 0 ]]; then
      echo "Encountered I/O error; setting the exit code accordingly";
      EXIT_CODE=3;
    fi

    if [[ $EXIT_CODE -ne 0 ]]; then
	  echo 'ERROR: hadd exited w/ non-zero return code. Will stop execution.'
	  cleanup "$JOB_DIR"
	  return $EXIT_CODE
    fi

    echo "Contents of temporary job dir: ls -laR $JOB_DIR"
    ls -laR $JOB_DIR

    OUTPUT_FILES="{{ outputFiles }}"
    echo "Copying output files: {{ outputFiles }}"
    for OUTPUT_FILE in $OUTPUT_FILES
    do
      # Check that input histograms are equal to output histogram
      check_that_histograms_are_equal.py $OUTPUT_FILE {{ inputFiles }}
      EXIT_CODE=$?

      if [[ $EXIT_CODE -ne 0 ]]; then
	    echo 'ERROR: Input histograms do not equal output histogram. Will stop execution.'
	    return 6;
      fi

      OUTPUT_DIR="{{ outputDir }}"
      if [[ "$OUTPUT_DIR" =~ ^/hdfs* && ( ! -z $(which hadoop 2>/dev/null) ) ]]; then
        echo "Hadoop commands available"
        cp_cmd="hadoop fs -copyFromLocal";
        st_cmd="hadoop fs -stat '%b'"
        ls_cmd="hadoop fs -ls"
        OUTPUT_DIR=${OUTPUT_DIR#/hdfs}
      else
        echo "Hadoop commands not available; resorting to POSIX commands"
        cp_cmd=cp;
        st_cmd="stat --printf='%s'"
        ls_cmd="ls -l"
      fi
      cp_cmd="$cp_cmd -f"

      OUTPUT_FILE_SIZE=$(stat -c '%s' $OUTPUT_FILE)
      if [ -n "$OUTPUT_FILE_SIZE" ] && [ $OUTPUT_FILE_SIZE -ge 1000 ]; then
        echo "$cp_cmd $OUTPUT_FILE $OUTPUT_DIR/$OUTPUT_FILE"

        CP_RETRIES=0
        COPIED=false
        while [ $CP_RETRIES -lt 3 ]; do
          CP_RETRIES=$[CP_RETRIES + 1];
          $cp_cmd $OUTPUT_FILE $OUTPUT_DIR/$OUTPUT_FILE

          # add a small delay before stat'ing the file
          sleep 5s

          REMOTE_SIZE=$($st_cmd $OUTPUT_DIR/$OUTPUT_FILE)
          if [ "$REMOTE_SIZE" == "$OUTPUT_FILE_SIZE" ]; then
            COPIED=true
            break;
          else
            continue;
          fi
        done

        if [ ! $COPIED ]; then
          EXIT_CODE=5;
        else
          $ls_cmd $OUTPUT_DIR/$OUTPUT_FILE
        fi

      else
        echo "$OUTPUT_FILE is broken, will exit with code 1."
        rm $OUTPUT_FILE
        EXIT_CODE=1
      fi
    done

    echo "End time is: `date`"

    cleanup "$JOB_DIR"

    echo "Waiting for 10 seconds before returning"
    sleep 10

    return $EXIT_CODE
}


# Calls main method

main

EXIT_CODE=$?
echo "Final exit code is: $EXIT_CODE"
exit $EXIT_CODE

# End of file
